# Identification-of-Key-Mode-Style
In this last decade, there have been monumental breakthroughs regarding the use of artificial intelligence (AI) in "human" fields, such as language and art. Coinciding with these advances in AI methods, also comes an unprecedented uptick in the social relevance of digital music, with the music streaming industry set to double in size within the next 15 years. Because of these developments, there is a growing need for software which is able to accurately classify, analyze, and manipulate digital music. This project attempts to develop these models, specifically to identify key, mode, and style, with the use of supervised machine learning (ML) models to an accuracy of at least 80% (engineering goal). First, the data– which came from a dataset of ~4,000 MIDI files– had to be extracted, cleaned, preprocessed, and augmented (balanced). Next, using various multiclass ML classifiers, ML models were trained to identify the key, mode, and style of a piece individually. Lastly, the most effective of the previous ML models underwent hyperparameter tuning in order to best optimize the model. The final ML models were able to identify the key, mode, and style of unseen data to an accuracy of ~95%, ~96%, and ~88%, respectively. Some applications of the aforementioned model include music classification (possibly commercially), music analysis, and the development of tonal algorithms for artificial music generation. In conclusion, the conditions of the engineering goal were satisfied as the final ML model was able to identify key, mode, and style around 93% of the time.

